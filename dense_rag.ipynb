{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = os.getenv('LANGCHAIN_ENDPOINT')\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_light = ChatGroq(model_name='llama3-8b-8192',\n",
    "                    temperature=0,\n",
    "                    streaming=True)\n",
    "\n",
    "# llm_light = Ollama(model='gemma2:2b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_q_optimization = \"\"\"You are a Query Optimizer designed to enhance user queries for use in a Retrieval-Augmented Generation (RAG) system. \n",
    "Your task is to transform user queries into optimized versions that retrieve the most relevant documents when processed with sentence transformers. \n",
    "Follow these rules:\n",
    "1. Only generate the optimized query. Do not include supporting text, explanations, or additional comments.\n",
    "2. Ensure the optimized query is concise, semantically rich, and tailored for high precision and relevance.\n",
    "\n",
    "Input: {input_}\"\"\"\n",
    "\n",
    "prompt_q_optimization_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", prompt_q_optimization),\n",
    "\n",
    "        (\"human\", \"What is the weather in Paris today?\"),\n",
    "        (\"ai\", \"weather Paris today\"),\n",
    "        \n",
    "        (\"human\", \"Show me recent advancements in artificial intelligence research.\"),\n",
    "        (\"ai\", \"recent advancements artificial intelligence research\"),\n",
    "        \n",
    "        (\"human\", \"Explain the use of transformers in deep learning.\"),\n",
    "        (\"ai\", \"transformers deep learning applications uses\"),\n",
    "        \n",
    "        (\"human\", \"Find me details about the Eiffel Tower.\"),\n",
    "        (\"ai\", \"Eiffel Tower details information\"),\n",
    "        \n",
    "        (\"human\", \"{input_}\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_q_optimization_template | llm_light | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_light = chain.invoke({'input_':'What is Pix2Pix++?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pix2Pix++ image-to-image translation deep learning'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_dense = ChatGroq(model_name='llama3-70b-8192',\n",
    "                    temperature=1,\n",
    "                    streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('portrait_to_pencil_sketch_draft_16.pdf')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "split_documents = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sentence-transformers\n",
      "Version: 3.3.1\n",
      "Summary: State-of-the-Art Text Embeddings\n",
      "Home-page: https://www.SBERT.net\n",
      "Author: \n",
      "Author-email: Nils Reimers <info@nils-reimers.de>, Tom Aarsen <tom.aarsen@huggingface.co>\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\humza\\anaconda3\\envs\\genai\\lib\\site-packages\n",
      "Requires: huggingface-hub, Pillow, scikit-learn, scipy, torch, tqdm, transformers\n",
      "Required-by: langchain-huggingface\n"
     ]
    }
   ],
   "source": [
    "! pip show sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Create the embeddings\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    cache_folder ='../cache'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = Chroma()\n",
    "vs = Chroma.from_documents(split_documents[0:35],embedding = embedding,persist_directory='db2',collection_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = chroma_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'weakref.ReferenceType' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretriever.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle 'weakref.ReferenceType' object"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"retriever.pkl\", \"wb\") as f:\n",
    "    pickle.dump(retriever, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_retrieval = ChatPromptTemplate.from_template(\"\"\"You are a precise and helpful AI assistant specializing in extracting and synthesizing relevant information from given context.\n",
    "\n",
    "Instructions:\n",
    "1. Carefully read the provided context.\n",
    "2. Answer the question directly and comprehensively using ONLY the information in the context.\n",
    "3. If the context does not contain sufficient information to fully answer the question, clearly state: \"I cannot find a complete answer in the provided context.\"\n",
    "4. Maintain the tone and level of detail present in the source context.\n",
    "5. If multiple relevant passages exist, synthesize information from across the context.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Response Guidelines:\n",
    "- Be concise and clear\n",
    "- Do not invent or assume any information not present in the context\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = retriever.invoke(chain.invoke({'input_':'Why is unet++ better than unet?'}))\n",
    "context_vanilla = retriever.invoke('Why is unet++ better than unet?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_2 = prompt_retrieval | llm_dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, U-Net++ is better than U-Net in the following ways:\n",
      "\n",
      "1. Dense connections: U-Net++ uses dense skip connections between immediate layers of the encoder and decoder, which allows the network to transfer fine-grained information between immediate layers more precisely. This enables the generator to produce high-resolution sketches compared to the vanilla U-Net.\n",
      "\n",
      "2. Deep supervision: U-Net++ provides deeper supervision training as it can include auxiliary inputs at intermediate stages, leading to faster and more stable convergence.\n",
      "\n",
      "These improvements in U-Net++ allow it to capture finer details and produce better results compared to the vanilla U-Net."
     ]
    }
   ],
   "source": [
    "async for token in chain_2.astream({'question':'Why is unet++ better than unet?','context':context}):\n",
    "    print(token.content,end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
